{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Bootstrap samples in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svani\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS CHAS    NOX     RM   AGE     DIS RAD    TAX  PTRATIO   \n",
       "0  0.00632  18.0   2.31    0  0.538  6.575  65.2  4.0900   1  296.0     15.3  \\\n",
       "1  0.02731   0.0   7.07    0  0.469  6.421  78.9  4.9671   2  242.0     17.8   \n",
       "2  0.02729   0.0   7.07    0  0.469  7.185  61.1  4.9671   2  242.0     17.8   \n",
       "3  0.03237   0.0   2.18    0  0.458  6.998  45.8  6.0622   3  222.0     18.7   \n",
       "4  0.06905   0.0   2.18    0  0.458  7.147  54.2  6.0622   3  222.0     18.7   \n",
       "\n",
       "        B  LSTAT  \n",
       "0  396.90   4.98  \n",
       "1  396.90   9.14  \n",
       "2  392.83   4.03  \n",
       "3  394.63   2.94  \n",
       "4  396.90   5.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'><b>Task 1</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> <b>Step - 1</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='blue'><b>Creating samples</b></font><br>\n",
    "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
    "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
    "    \n",
    "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]\n",
    "* <font color='blue'><b> Create 30 samples </b></font>\n",
    "    *  Note that as a part of the Bagging when you are taking the random samples <b>make sure each of the sample will have different set of columns</b><br>\n",
    "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
    "Make sure each sample will have atleast 3 feautres/columns/attributes\n",
    "\n",
    "* <font color='red'><b> Note - While selecting the random 60% datapoints from the whole data, make sure that the selected datapoints are all exclusive, repetition is not allowed. </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_samples(input_data, target_data): \n",
    "    \n",
    "    np.random.seed(123) # Set random seed\n",
    "\n",
    "    # random.choice to generate random indices without replacement\n",
    "    # Selecting_rows => Getting 303 random row indices from the input_data, without replacement\n",
    "    Selecting_rows = input_data.sample(n=303, replace=False).index\n",
    "\n",
    "    # now we will replicate 203 points from above selected rows\n",
    "    # Replacing Rows => Extracting 206 random row indices from the Selecting_rows\n",
    "    Replacing_rows = pd.Series(Selecting_rows).sample(n=203, replace=False).values\n",
    "\n",
    "    # Selecting_columns => Getting 3 to 13 random column indices from input_data\n",
    "    Selecting_columns = random.randint(3, 13)\n",
    "    columns_selected = np.array(random.sample(range(0, 13), Selecting_columns ))\n",
    "\n",
    "    # sample_data => input_data.iloc[Selecting_rows[:,None], Selecting_columns]\n",
    "    sample_data = input_data.iloc[Selecting_rows, columns_selected]\n",
    "    \n",
    "    # target_of_sample_data => target_data[Selecting_rows]\n",
    "    target_of_sample_data = target_data[Selecting_rows]\n",
    "    \n",
    "    # Replicating data\n",
    "    # Now Replication of Data for 203 data points out of 303 selected points\n",
    "    replicated_sample_data = input_data.iloc[Replacing_rows, columns_selected ]\n",
    "    target_of_replicated_sample_data = target_data[Replacing_rows]\n",
    "    \n",
    "    # Concatenating data\n",
    "    final_sample_data = np.vstack((sample_data.values, replicated_sample_data.values ))\n",
    "    final_target_data = np.vstack((target_of_sample_data.to_numpy().reshape(-1, 1), target_of_replicated_sample_data.to_numpy().reshape(-1, 1)))\n",
    "            \n",
    "    return final_sample_data, final_target_data, Selecting_rows, columns_selected\n",
    "\n",
    "a,b,c,d = generating_samples(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 11) (506, 1) (303,) (11,)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape, b.shape, c.shape, d.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='cyan'> <b> Grader function - 1 </b> </fongt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_samples(a,b,c,d):\n",
    "    length = (len(a)==506  and len(b)==506)\n",
    "    sampled = (len(a)-len(set([str(i) for i in a]))==203)\n",
    "    rows_length = (len(c)==303)\n",
    "    column_length= (len(d)>=3)\n",
    "    assert(length and sampled and rows_length and column_length)\n",
    "    return True\n",
    "grader_samples(a,b,c,d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='blue'> <b>Create 30 samples </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the samples\n",
    "list_input_data = []\n",
    "list_output_data = []\n",
    "list_selected_row = []\n",
    "list_selected_columns = []\n",
    "\n",
    "# Loop to generate 30 samples\n",
    "for i in range(30):\n",
    "    # Generate a sample\n",
    "    a, b, c, d = generating_samples(x, y)\n",
    "    \n",
    "    # Append the sample to the respective lists\n",
    "    list_input_data.append(a)\n",
    "    list_output_data.append(b)\n",
    "    list_selected_row.append(c)\n",
    "    list_selected_columns.append(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='cyan'> <b>Grader function - 2 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_30(a):\n",
    "    assert(len(a)==30 and len(a[0])==506)\n",
    "    return True\n",
    "grader_30(list_input_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Step - 2 </b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Building High Variance Models on each of the sample and finding train MSE value</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Build a regression trees on each of 30 samples.\n",
    "*  Computed the predicted values of each data point(506 data points) in your corpus.\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
    "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the trained models\n",
    "tree_models = []\n",
    "\n",
    "# Train a decision tree model for each of the 30 samples\n",
    "for i in range(30):\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(list_input_data[i], list_output_data[i])\n",
    "    \n",
    "    # Append the trained model to the list\n",
    "    tree_models.append(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor(), DecisionTreeRegressor()]\n"
     ]
    }
   ],
   "source": [
    "print(tree_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for each model and store the predicted values in a list\n",
    "list_predicted_values = []\n",
    "for i in range(30):\n",
    "    # Predict the output values for the i-th sample using the corresponding model\n",
    "    predicted_values = tree_models[i].predict(list_input_data[i])\n",
    "    list_predicted_values.append(predicted_values)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting predicted_y for each data point, we can use sklearns mean_squared_error to calculate the MSE between predicted_y and actual_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "--------------------------------------------------\n",
      "[0.0, 0.0, 0.0, 0.0, 7.483265504072839e-32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06800395256917001, 7.483265504072839e-32, 0.0, 0.0, 0.0, 0.0, 7.483265504072839e-32, 0.0, 7.483265504072839e-32, 0.0, 0.0, 0.0, 0.0, 7.483265504072839e-32, 0.0, 7.483265504072839e-32, 7.483265504072839e-32, 0.0, 0.0]\n",
      "--------------------------------------------------\n",
      "mean of the mean squared errors:  0.002266798418972334\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean squared error for each sample's predictions and store them in a list\n",
    "\n",
    "mse_values = []\n",
    "for i in range(len(list_predicted_values)):\n",
    "    # Calculate the mean squared error between the i-th sample's actual output and predicted output\n",
    "    mse = mean_squared_error(list_output_data[i], list_predicted_values[i])\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "\n",
    "print(len(mse_values))\n",
    "print('-'*50)\n",
    "\n",
    "print(mse_values)\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "# Calculate the mean of the mean squared errors\n",
    "mean_mse = np.mean(mse_values)\n",
    "print('mean of the mean squared errors: ', mean_mse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that, the mean squared error (MSE) values for the predicted house prices compared to the actual prices are remarkably low. This indicates that the decision tree models I trained on the Boston Housing dataset are performing exceptionally well in predicting the target variable. The MSE measures the average squared difference between the predicted and actual values, and a lower MSE signifies superior model performance.\n",
    "\n",
    "Upon examining the MSE values, it is evident that the majority of them are very close to zero. This implies that the models have successfully captured the underlying patterns and relationships within the dataset, enabling them to make highly accurate predictions. The models have effectively learned from the input features and established strong correlations with the target variable.\n",
    "\n",
    "By calculating the mean of the MSE values, I obtained a value of 0.0002905138339920953. \n",
    "This further reinforces the overall outstanding performance of the models. A low mean MSE indicates that, on average, the predicted house prices closely align with the actual prices.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> <b>Step - 3 </b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='blue'><b>Calculating the OOB score </b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.\n",
    "*  Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[[1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505], [1, 2, 3, 8, 14, 16, 17, 18, 25, 27, 32, 39, 40, 43, 45, 46, 47, 50, 51, 56, 57, 58, 60, 62, 63, 64, 65, 67, 68, 70, 73, 76, 77, 83, 84, 86, 87, 88, 92, 96, 97, 98, 99, 100, 103, 106, 109, 110, 111, 112, 113, 116, 118, 119, 122, 123, 126, 129, 130, 133, 135, 137, 139, 140, 141, 146, 149, 153, 154, 158, 168, 169, 174, 176, 180, 183, 186, 187, 193, 195, 197, 198, 205, 206, 208, 213, 214, 215, 222, 224, 225, 230, 233, 238, 243, 244, 247, 250, 251, 253, 254, 255, 256, 257, 259, 262, 265, 268, 269, 270, 271, 278, 281, 283, 290, 294, 296, 300, 301, 302, 303, 304, 305, 307, 311, 312, 314, 315, 319, 321, 322, 323, 324, 325, 330, 331, 334, 336, 339, 340, 342, 348, 349, 350, 353, 354, 355, 357, 358, 359, 360, 361, 363, 365, 366, 371, 373, 377, 380, 382, 386, 387, 390, 391, 394, 395, 396, 398, 400, 404, 406, 409, 411, 414, 418, 420, 424, 428, 430, 438, 439, 445, 451, 453, 454, 459, 462, 464, 471, 473, 475, 476, 477, 480, 488, 489, 492, 496, 497, 499, 503, 504, 505]]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the OOB indices for each tree\n",
    "oob_indices_per_tree = []\n",
    "\n",
    "for i in range(30):\n",
    "    # Get the indices of the rows that were not selected for the i-th sample\n",
    "    oob_indices = list(set(range(506)) - set(list_selected_row[i]))\n",
    "    \n",
    "    # Append the indices to the list\n",
    "    oob_indices_per_tree.append(oob_indices)\n",
    "\n",
    "print(len(oob_indices_per_tree))\n",
    "print(oob_indices_per_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the OOB predictions for each tree\n",
    "oob_predictions_per_tree = []\n",
    "\n",
    "for i in range(30):\n",
    "    # Get the indices of the rows that were not selected for the i-th sample\n",
    "    oob_indices = oob_indices_per_tree[i]\n",
    "    \n",
    "    # Get the input data corresponding to the OOB indices\n",
    "    oob_input_data = x.iloc[oob_indices, :]\n",
    "    \n",
    "    # Select only the columns used for training the i-th model\n",
    "    oob_input_data = oob_input_data.iloc[:, list_selected_columns[i]]\n",
    "    \n",
    "    # Get the predictions on the OOB input data using the i-th model\n",
    "    oob_predictions = tree_models[i].predict(oob_input_data)\n",
    "\n",
    "    # Append the predictions to the list\n",
    "    oob_predictions_per_tree.append(oob_predictions)\n",
    "\n",
    "print(len(oob_predictions_per_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "--------------------------------------------------\n",
      "[52.93014778325123, 23.724876847290638, 31.650492610837436, 17.05128078817734, 26.342315270935963, 15.675418719211823, 73.74600985221674, 19.52857142857143, 49.99733990147784, 19.23295566502463, 62.0695566502463, 30.308091133004925, 40.020640394088666, 42.55935960591133, 30.76443349753694, 16.365320197044333, 29.15630541871921, 19.05192118226601, 71.77669950738915, 18.569901477832513, 19.03408866995074, 25.45729064039409, 21.533743842364533, 32.60310344827587, 19.008768472906404, 19.973152709359603, 43.82600985221675, 19.64576354679803, 40.15049261083744, 78.75039408866995]\n",
      "--------------------------------------------------\n",
      "mean of the OOB MSE values:  33.68348152709359\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the OOB MSE values for each tree\n",
    "oob_mse_values = []\n",
    "\n",
    "# iterate over each tree for calculating the OOB MSE\n",
    "for i in range(30):\n",
    "    # Get the OOB indices for the i-th tree\n",
    "    oob_indices = oob_indices_per_tree[i]\n",
    "    \n",
    "    # Get the actual output values corresponding to the OOB indices\n",
    "    actual_output = y[oob_indices]\n",
    "    \n",
    "    # Get the OOB predictions for the i-th tree\n",
    "    oob_predictions = oob_predictions_per_tree[i]\n",
    "    \n",
    "    # Calculate the OOB MSE for the i-th tree\n",
    "    oob_mse = mean_squared_error(actual_output, oob_predictions)\n",
    "    \n",
    "    # Append the OOB MSE to the list\n",
    "    oob_mse_values.append(oob_mse)\n",
    "    \n",
    "print(len(oob_mse_values))\n",
    "print('-'*50)\n",
    "\n",
    "print(oob_mse_values)\n",
    "print('-'*50)\n",
    "\n",
    "# Calculate the mean of the OOB MSE values\n",
    "mean_oob_mse = np.mean(oob_mse_values)\n",
    "print('mean of the OOB MSE values: ', mean_oob_mse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the out-of-bag mean squared error (OOB MSE) values for each tree in the random forest are varied, ranging from 17.247881773399012 to 76.57571428571427.\n",
    "\n",
    "After calculating the OOB MSE values for all the trees in the random forest, I obtained a mean OOB MSE of 30.2547816091954. This value indicates the overall performance of the random forest model in predicting house prices.\n",
    "\n",
    "Considering the range of OOB MSE values and the mean value, it can be concluded that the random forest model exhibits some variability in its performance across different trees. While some trees achieve relatively low OOB MSE values, indicating accurate predictions, others have higher OOB MSE values, indicating less accurate predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'><b>Task 2</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='blue'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "  *   Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "num_samples = 30\n",
    "num_data_points = 506\n",
    "num_repetitions = 35\n",
    "\n",
    "# Create an empty list to store the trained models\n",
    "tree_models = []\n",
    "\n",
    "# Train a decision tree model for each of the 30 samples\n",
    "for i in range(num_samples):\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(list_input_data[i], list_output_data[i])\n",
    "    \n",
    "    # Append the trained model to the list\n",
    "    tree_models.append(tree_model)\n",
    "\n",
    "# Create empty lists to store the MSE scores and OOB scores for each iteration\n",
    "list_mse_scores = []\n",
    "list_oob_scores = []\n",
    "\n",
    "for iteration in range(num_repetitions):\n",
    "    # Make predictions for each model and store the predicted values in a list\n",
    "    list_predicted_values = []\n",
    "    for i in range(num_samples):\n",
    "        # Predict the output values for the i-th sample using the corresponding model\n",
    "        predicted_values = tree_models[i].predict(list_input_data[i])\n",
    "        list_predicted_values.append(predicted_values)\n",
    "\n",
    "    # Calculate the mean squared error for each sample's predictions and store them in a list\n",
    "    mse_values = []\n",
    "    for i in range(len(list_predicted_values)):\n",
    "        # Calculate the mean squared error between the i-th sample's actual output and predicted output\n",
    "        mse = mean_squared_error(list_output_data[i], list_predicted_values[i])\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    # Append the MSE scores to the list\n",
    "    list_mse_scores.append(mse_values)\n",
    "\n",
    "    # Create an empty list to store the OOB indices for each tree\n",
    "    oob_indices_per_tree = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Get the indices of the rows that were not selected for the i-th sample\n",
    "        oob_indices = list(set(range(num_data_points)) - set(list_selected_row[i]))\n",
    "\n",
    "        # Append the indices to the list\n",
    "        oob_indices_per_tree.append(oob_indices)\n",
    "\n",
    "    # Create an empty list to store the OOB predictions for each tree\n",
    "    oob_predictions_per_tree = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Get the indices of the rows that were not selected for the i-th sample\n",
    "        oob_indices = oob_indices_per_tree[i]\n",
    "\n",
    "        # Get the input data corresponding to the OOB indices\n",
    "        oob_input_data = x.iloc[oob_indices, :]\n",
    "\n",
    "        # Select only the columns used for training the i-th model\n",
    "        oob_input_data = oob_input_data.iloc[:, list_selected_columns[i]]\n",
    "\n",
    "        # Get the predictions on the OOB input data using the i-th model\n",
    "        oob_predictions = tree_models[i].predict(oob_input_data)\n",
    "\n",
    "        # Append the predictions to the list\n",
    "        oob_predictions_per_tree.append(oob_predictions)\n",
    "\n",
    "    # Create an empty list to store the OOB MSE values for each tree\n",
    "    oob_mse_values = []\n",
    "\n",
    "    # Iterate over each tree for calculating the OOB MSE\n",
    "    for i in range(num_samples):\n",
    "        # Get the OOB indices for the i-th tree\n",
    "        oob_indices = oob_indices_per_tree[i]\n",
    "\n",
    "        # Get the actual output values corresponding to the OOB indices\n",
    "        actual_output = y[oob_indices]\n",
    "\n",
    "        # Get the OOB predictions for the i-th tree\n",
    "        oob_predictions = oob_predictions_per_tree[i]\n",
    "\n",
    "        # Calculate the OOB MSE for the i-th tree\n",
    "        oob_mse = mean_squared_error(actual_output, oob_predictions)\n",
    "\n",
    "        # Append the OOB MSE to the list\n",
    "        oob_mse_values.append(oob_mse)\n",
    "\n",
    "    # Append the OOB scores to the list\n",
    "    list_oob_scores.append(oob_mse_values)\n",
    "    \n",
    "print(len(list_mse_scores))\n",
    "print(len(list_oob_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Mean: 0.0023\n",
      "MSE Standard Deviation: 0.0122\n",
      "MSE Confidence Interval: [-0.0018, 0.0063]\n",
      "---\n",
      "OOB Mean: 33.9146\n",
      "OOB Standard Deviation: 18.6490\n",
      "OOB Confidence Interval: [27.7363, 40.0930]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE\n",
    "mse_mean = np.mean(list_mse_scores)\n",
    "mse_std = np.std(list_mse_scores)\n",
    "\n",
    "# Calculate the mean and standard deviation of OOB scores\n",
    "oob_mean = np.mean(list_oob_scores)\n",
    "oob_std = np.std(list_oob_scores)\n",
    "\n",
    "# Set the desired confidence level (e.g., 95%)\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Calculate the critical value based on the desired confidence level\n",
    "critical_value = norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "\n",
    "# Calculate the margin of error for MSE\n",
    "mse_margin_of_error = critical_value * (mse_std / np.sqrt(num_repetitions))\n",
    "\n",
    "# Calculate the confidence interval for MSE\n",
    "mse_ci_lower = mse_mean - mse_margin_of_error\n",
    "mse_ci_upper = mse_mean + mse_margin_of_error\n",
    "\n",
    "# Calculate the margin of error for OOB scores\n",
    "oob_margin_of_error = critical_value * (oob_std / np.sqrt(num_repetitions))\n",
    "\n",
    "# Calculate the confidence interval for OOB scores\n",
    "oob_ci_lower = oob_mean - oob_margin_of_error\n",
    "oob_ci_upper = oob_mean + oob_margin_of_error\n",
    "\n",
    "# Print the results\n",
    "print(\"MSE Mean: {:.4f}\".format(mse_mean))\n",
    "print(\"MSE Standard Deviation: {:.4f}\".format(mse_std))\n",
    "print(\"MSE Confidence Interval: [{:.4f}, {:.4f}]\".format(mse_ci_lower, mse_ci_upper))\n",
    "print(\"---\")\n",
    "print(\"OOB Mean: {:.4f}\".format(oob_mean))\n",
    "print(\"OOB Standard Deviation: {:.4f}\".format(oob_std))\n",
    "print(\"OOB Confidence Interval: [{:.4f}, {:.4f}]\".format(oob_ci_lower, oob_ci_upper))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mean squared error (MSE), I got a confidence level of 95%, the confidence interval for the MSE is estimated to be [-0.0002, 0.0008]. This interval provides a range within which the true population mean MSE is likely to fall.\n",
    "Also, the average MSE is 0.0003 with a standard deviation of 0.0016.\n",
    "\n",
    "While, the out-of-bag (OOB) scores, shows that the average OOB score is 30.0754 with a standard deviation of 16.3830. \n",
    "The confidence interval for the OOB scores is [24.6478, 35.5030] at a 95% confidence level."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the MSE and OOB MSE results, it is evident that the magnitudes of the values differ significantly. \n",
    "\n",
    "The MSE measures the performance of the individual decision tree models on their respective training samples, while the OOB MSE evaluates the performance of the random forest model using the out-of-bag samples. \n",
    "\n",
    "The MSE values are substantially lower (0.0003) compared to the OOB MSE values (30.0754), indicating that the individual decision tree models tend to overfit the training data. \n",
    "\n",
    "This suggests that the random forest model achieves better generalization by considering the OOB samples, resulting in slightly higher MSE values but improved predictive performance overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'><b>Task 3</b></font>\n",
    "\n",
    "*  <font color='blue'><b>Given a single query point predict the price of house.\n",
    "\n",
    "\n",
    "\n",
    "Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
    "Predict the house price for this point as mentioned in the step 2 of Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted House Price: 18.533333333333335\n"
     ]
    }
   ],
   "source": [
    "xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60]\n",
    "\n",
    "predicted_values = []\n",
    "\n",
    "# Iterate over each model and make predictions for the data point xq\n",
    "for i in range(len(tree_models)):\n",
    "    # Get the selected columns for the i-th model\n",
    "    selected_columns = list_selected_columns[i]\n",
    "    \n",
    "    # Select the corresponding features from xq\n",
    "    xq_selected = [xq[j] for j in selected_columns]\n",
    "    \n",
    "    # Make a prediction using the i-th model\n",
    "    prediction = tree_models[i].predict([xq_selected])\n",
    "    \n",
    "    # Append the prediction to the list of predicted values\n",
    "    predicted_values.append(prediction)\n",
    "\n",
    "# Calculate the average predicted value\n",
    "average_prediction = np.mean(predicted_values)\n",
    "\n",
    "# Print the predicted house price\n",
    "print(\"Predicted House Price:\", average_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is obtained by applying the trained decision tree models to the data point xq. Each decision tree model selects specific features and makes an individual prediction. The predicted values from all the models are then averaged to obtain the final prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
