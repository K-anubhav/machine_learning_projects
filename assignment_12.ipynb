{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Bootstrap samples in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svani\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:301: UserWarning: Multiple active versions of the dataset matching the name boston exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n",
      "C:\\Users\\svani\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # importing numpy for numerical computation\n",
    "# here we are using sklearn's boston dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "boston = fetch_openml(name='boston')\n",
    "from sklearn.metrics import mean_squared_error # importing mean_squared_error metric\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'><b>Task 1</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> <b>Step - 1</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='blue'><b>Creating samples</b></font><br>\n",
    "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
    "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
    "    \n",
    "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]\n",
    "* <font color='blue'><b> Create 30 samples </b></font>\n",
    "    *  Note that as a part of the Bagging when you are taking the random samples <b>make sure each of the sample will have different set of columns</b><br>\n",
    "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
    "Make sure each sample will have atleast 3 feautres/columns/attributes\n",
    "\n",
    "* <font color='red'><b> Note - While selecting the random 60% datapoints from the whole data, make sure that the selected datapoints are all exclusive, repetition is not allowed. </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "\n",
    "def generating_samples(input_data, target_data): \n",
    "    \n",
    "    np.random.seed(123) # Set random seed\n",
    "\n",
    "    # random.choice to generate random indices without replacement\n",
    "    # Selecting_rows => Getting 303 random row indices from the input_data, without replacement\n",
    "    Selecting_rows = input_data.sample(n=303, replace=False).index\n",
    "\n",
    "    # now we will replicate 203 points from above selected rows\n",
    "    # Replacing Rows => Extracting 206 random row indices from the Selecting_rows\n",
    "    Replacing_rows = pd.Series(Selecting_rows).sample(n=203, replace=False).values\n",
    "\n",
    "    # Selecting_columns => Getting 3 to 13 random column indices from input_data\n",
    "    Selecting_columns = random.randint(3, 13)\n",
    "    columns_selected = np.array(random.sample(range(0, 13), Selecting_columns ))\n",
    "\n",
    "    # sample_data => input_data.iloc[Selecting_rows[:,None], Selecting_columns]\n",
    "    sample_data = input_data.iloc[Selecting_rows, columns_selected]\n",
    "    \n",
    "    # target_of_sample_data => target_data[Selecting_rows]\n",
    "    target_of_sample_data = target_data[Selecting_rows]\n",
    "    \n",
    "    # Replicating data\n",
    "    # Now Replication of Data for 203 data points out of 303 selected points\n",
    "    replicated_sample_data = input_data.iloc[Replacing_rows, columns_selected ]\n",
    "    target_of_replicated_sample_data = target_data[Replacing_rows]\n",
    "    \n",
    "    # Concatenating data\n",
    "    final_sample_data = np.vstack((sample_data.values, replicated_sample_data.values ))\n",
    "    final_target_data = np.vstack((target_of_sample_data.to_numpy().reshape(-1, 1), target_of_replicated_sample_data.to_numpy().reshape(-1, 1)))\n",
    "            \n",
    "    return final_sample_data, final_target_data, Selecting_rows, columns_selected\n",
    "\n",
    "a,b,c,d = generating_samples(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='cyan'> <b> Grader function - 1 </b> </fongt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_samples(a,b,c,d):\n",
    "    length = (len(a)==506  and len(b)==506)\n",
    "    sampled = (len(a)-len(set([str(i) for i in a]))==203)\n",
    "    rows_length = (len(c)==303)\n",
    "    column_length= (len(d)>=3)\n",
    "    assert(length and sampled and rows_length and column_length)\n",
    "    return True\n",
    "a,b,c,d = generating_samples(x, y)\n",
    "grader_samples(a,b,c,d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='blue'> <b>Create 30 samples </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the samples\n",
    "list_input_data = []\n",
    "list_output_data = []\n",
    "list_selected_row = []\n",
    "list_selected_columns = []\n",
    "\n",
    "# Loop to generate 30 samples\n",
    "for i in range(30):\n",
    "    # Generate a sample\n",
    "    a, b, c, d = generating_samples(x, y)\n",
    "    \n",
    "    # Append the sample to the respective lists\n",
    "    list_input_data.append(a)\n",
    "    list_output_data.append(b)\n",
    "    list_selected_row.append(c)\n",
    "    list_selected_columns.append(d)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='cyan'> <b>Grader function - 2 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_30(a):\n",
    "    assert(len(a)==30 and len(a[0])==506)\n",
    "    return True\n",
    "grader_30(list_input_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Step - 2 </b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Building High Variance Models on each of the sample and finding train MSE value</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Build a regression trees on each of 30 samples.\n",
    "*  Computed the predicted values of each data point(506 data points) in your corpus.\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
    "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create an empty list to store the trained models\n",
    "tree_models = []\n",
    "\n",
    "# Train a decision tree model for each of the 30 samples\n",
    "for i in range(30):\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(list_input_data[i], list_output_data[i])\n",
    "    \n",
    "    # Append the trained model to the list\n",
    "    tree_models.append(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for each model and store the predicted values in a list\n",
    "list_predicted_values = []\n",
    "for i in range(len(tree_models)):\n",
    "    # Predict the output values for the i-th sample using the corresponding model\n",
    "    predicted_values = tree_models[i].predict(list_input_data[i])\n",
    "    list_predicted_values.append(predicted_values)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting predicted_y for each data point, we can use sklearns mean_squared_error to calculate the MSE between predicted_y and actual_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 7.483265504072839e-32, 7.483265504072839e-32, 7.483265504072839e-32, 0.0, 0.0, 7.483265504072839e-32, 0.0, 0.16527009222661393, 7.483265504072839e-32, 0.0, 1.9410724637681158, 7.483265504072839e-32, 7.483265504072839e-32, 0.0, 0.0, 0.0, 7.483265504072839e-32, 0.0, 0.06449440052700924, 0.0, 0.0, 0.19963109354413697, 0.16527009222661393, 0.0, 7.483265504072839e-32, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean squared error for each sample's predictions and store them in a list\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_values = []\n",
    "for i in range(len(list_predicted_values)):\n",
    "    # Calculate the mean squared error between the i-th sample's actual output and predicted output\n",
    "    mse = mean_squared_error(list_output_data[i], list_predicted_values[i])\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "print(mse_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step - 3 </b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the indices of samples that do not include a data point\n",
    "oob_indices = []\n",
    "\n",
    "for i in range(506):\n",
    "    # Identify the indices of all samples that do not include the i-th data point\n",
    "    sample_indices = []\n",
    "    for j in range(len(list_input_data)):\n",
    "        if i not in list_input_data[j]:\n",
    "            sample_indices.append(j)\n",
    "    # Append these indices to the list of OOB indices\n",
    "    oob_indices.append(sample_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall OOB score: 67.0459486166008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create an empty list to store the OOB scores\n",
    "oob_scores = []\n",
    "\n",
    "for i in range(506):\n",
    "    # Identify the indices of all samples that do not include the i-th data point\n",
    "    sample_indices = oob_indices[i]\n",
    "    # Use these samples as the training data to build the regression tree model\n",
    "    X_train = a[sample_indices]\n",
    "    y_train = b[sample_indices]\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    # Use the data points that were not included in the subset as the OOB data\n",
    "    oob_data = a[i].reshape(1, -1)\n",
    "    # Compute the predicted value of the target variable for the OOB data\n",
    "    y_pred = model.predict(oob_data)\n",
    "    # Compute the OOB score for the i-th data point\n",
    "    y_actual = b[i]\n",
    "    oob_score = (y_pred - y_actual) ** 2\n",
    "    # Append the OOB score to the list of OOB scores\n",
    "    oob_scores.append(oob_score)\n",
    "\n",
    "# Compute the overall OOB score by averaging the OOB scores for all 506 data points\n",
    "mean_oob_score = np.mean(oob_scores)\n",
    "print(\"Overall OOB score:\", mean_oob_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'><b>Task 2</b></font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='blue'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "  *   Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
